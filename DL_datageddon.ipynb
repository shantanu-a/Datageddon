{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NPnvGZrfe_xi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fH1Bdo7pfBbq"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Datageddon/X.npy'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Data\\Datageddon\\DL_datageddon.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Data/Datageddon/DL_datageddon.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mDatageddon/X.npy\u001b[39;49m\u001b[39m'\u001b[39;49m)  \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Data/Datageddon/DL_datageddon.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDatageddon/Y.npy\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\shnga\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    403\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 405\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os_fspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m    406\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Datageddon/X.npy'"
          ]
        }
      ],
      "source": [
        "X = np.load('C:\\Data\\Datageddon/X.npy')  \n",
        "y = np.load('C:\\Data\\Datageddon/Y.npy')  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3R7qVMKJfDaV",
        "outputId": "f74e93f7-41d8-48c7-89ac-cae231d8992c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(24000, 64, 3, 3, 3) (24000,)\n"
          ]
        }
      ],
      "source": [
        "print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "-LCUShNcfFaI"
      },
      "outputs": [],
      "source": [
        "# You may change the train-test split according to your will\n",
        "X_train, y_train = X[:20000], y[:20000]\n",
        "X_val, y_val = X[20000:], y[20000:]\n",
        "#_val variables are basically test data variables\n",
        "\n",
        "\n",
        "X_train = torch.from_numpy(X_train).float()  \n",
        "y_train = torch.from_numpy(y_train).long()   \n",
        "X_val = torch.from_numpy(X_val).float()  \n",
        "y_val = torch.from_numpy(y_val).long().reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "yJRSHKasfHjt"
      },
      "outputs": [],
      "source": [
        "#batch size is the number of attributes processed until the model is updated\n",
        "batch_size = 32\n",
        "\n",
        "#Dataloader is used to load our data while Dataset is used to wrap an iterable around it to allow easy access\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vAaBPAIBd3f",
        "outputId": "4cbd7cfe-1154-4c02-ae61-bbfd2bb14cc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=1728, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=4, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "#Actual implementation of deep neural network(3 layered DL)\n",
        "#Activation function used is relu i.e max(0,x)\n",
        "import torch.nn.functional as F\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net,self).__init__()\n",
        "    self.fc1=nn.Linear(64*3*3*3,512)\n",
        "    self.fc2=nn.Linear(512,64)\n",
        "    self.fc3=nn.Linear(64,4)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=x.view(-1,64*3*3*3).clone()\n",
        "    x=F.relu(self.fc1(x))\n",
        "    x=F.relu(self.fc2(x))\n",
        "    x=self.fc3(x)\n",
        "    return x\n",
        "model=Net()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "WxsfTaMzEFCE"
      },
      "outputs": [],
      "source": [
        "#We are optimizing our DL network and defining loss function.\n",
        "import torch.optim as optim\n",
        "loss_fn=nn.CrossEntropyLoss()\n",
        "optimizer=optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbWXkzD2IlFL",
        "outputId": "52ac143d-5b67-41c1-af55-74696f040e82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 loss :  14.336879834532738\n",
            "Epoch 2 loss :  12.238405227661133\n",
            "Epoch 3 loss :  12.049834985285997\n",
            "Epoch 4 loss :  12.04219182766974\n",
            "Epoch 5 loss :  12.004976166877896\n",
            "Epoch 6 loss :  12.006633497774601\n",
            "Epoch 7 loss :  11.98194840643555\n",
            "Epoch 8 loss :  11.974398431368172\n",
            "Epoch 9 loss :  11.96659620385617\n",
            "Epoch 10 loss :  11.969770342111588\n",
            "Epoch 11 loss :  11.96582920383662\n",
            "Epoch 12 loss :  11.957969003356993\n",
            "Epoch 13 loss :  11.96532277110964\n",
            "Epoch 14 loss :  11.970296154730022\n",
            "Epoch 15 loss :  11.963701010681689\n",
            "Epoch 16 loss :  11.961249224841595\n",
            "Epoch 17 loss :  11.963183560408652\n",
            "Epoch 18 loss :  11.960808269679546\n",
            "Epoch 19 loss :  11.963963507674634\n",
            "Epoch 20 loss :  11.960436001420021\n"
          ]
        }
      ],
      "source": [
        "#This is the code for training our neural network. I am printing the loss for each epoch to see that the loss is reducing.\n",
        "i=1\n",
        "for epoch in range(20):\n",
        "    train_loss = 0.\n",
        "    for local_batch, local_labels in train_dataloader:\n",
        "        y_pred = model(local_batch)\n",
        "        loss = loss_fn(y_pred,local_labels)\n",
        "        train_loss += loss.item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    train_loss/=batch_size\n",
        "    print(f\"Epoch {i} loss : \", train_loss)\n",
        "    i+=1\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHCotziycGML",
        "outputId": "19ddbfa1-41e1-4598-d387-eb02a115bbfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy 71.71000000000075\n"
          ]
        }
      ],
      "source": [
        "# Accuracy of training dataset\n",
        "acc = 0.\n",
        "count = 0\n",
        "with torch.set_grad_enabled(False):\n",
        "  for local_batch, local_labels in train_dataloader:\n",
        "    #predict the label\n",
        "    y_pred = model(local_batch)\n",
        "    #find the index of the element with max weight which will give us the label.\n",
        "    y_pred = torch.argmax(y_pred, dim = 1)\n",
        "    #then we check if that label is same as the given label(for accuracy measurements)\n",
        "\n",
        "    for i in range(local_labels.shape[0]):\n",
        "      if local_labels[i] == y_pred[i]:\n",
        "        acc = 1./(count+1) + acc*count/(count + 1)\n",
        "      else:\n",
        "        acc = acc*count/(count + 1)\n",
        "      count += 1\n",
        "    \n",
        "print(f\"Accuracy {acc*100}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W64tTBo9KBmw",
        "outputId": "4b73fdb4-f602-43a1-d5e5-e4b4ff4e8ea3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy 73.17499999999984\n"
          ]
        }
      ],
      "source": [
        "# Accuracy of validation dataset\n",
        "acc = 0.\n",
        "count = 0\n",
        "with torch.set_grad_enabled(False):\n",
        "  for local_batch, local_labels in val_dataloader:\n",
        "    y_pred = model(local_batch)\n",
        "    y_pred = torch.argmax(y_pred, dim = 1)\n",
        "\n",
        "    for i in range(local_labels.shape[0]):\n",
        "      if local_labels[i] == y_pred[i]:\n",
        "        acc = 1./(count+1) + acc*count/(count + 1)\n",
        "      else:\n",
        "        acc = acc*count/(count + 1)\n",
        "      count += 1\n",
        "    \n",
        "print(f\"Accuracy {acc*100}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "YKbAdhtFdRZk"
      },
      "outputs": [],
      "source": [
        "#saving the weights and other parameters for the model\n",
        "torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, '/content/drive/MyDrive/Datageddon_files/weights.pt')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
